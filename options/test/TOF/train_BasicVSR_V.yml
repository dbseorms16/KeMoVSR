#### general settings
name: basic2020_v4
use_tb_logger: true
model: video_base
distortion: sr
scale: 4
gpu_ids: [0] # [0,1,2,3,4,5,6,7]
kernel_size : 21
sigma_x: 2.0
sigma_y: 2.0
theta: 0
#### datasets
datasets:
  train:
    name: Vimeo
    mode: Vimeo
    interval_list: [1]
    random_reverse: false
    border_mode: false
    data_root: 'F:\\'
    img_type: lmdb
    N_frames: 7
    use_shuffle: true
    n_workers: 8  # per GPU
    batch_size: 2
    patch_size: 64
    use_flip: true
    use_rot: true
    kernel_size: 21
    color: RGB
  val:
    name: vid4
    mode: video_test
    # dataroot_GT: F:/Vid4_GTx2
    
    # dataroot_GT: F:/Vid4/GT_test
    dataroot_GT: /home/jsyun/project/VSR_data/test_realdata/
    dataroot_LQ: /home/jsyun/project/VSR_data/test_realdata/
    cache_data: True
    N_frames: 7
    padding: new_info
    degradation_mode: set
    degradation_type: impulse
    sigma_x: 0.2
    sigma_y: 0.2
    theta: 0

#### network structures
network_G:
  which_model_G: BasicVSRplus
  nf: 64
  nframes: 7
  groups: 8
  front_RBs: 5
  back_RBs: 10
  predeblur: false
  HR_in: false
  w_TSA: true

network_E:
  which_model_E: MFDN
  mode: video
  nf: 64
  in_nc: 3
#### path
path:
  # bicubic_G: F:\DynaVSR-master\experiments\Basic_VSR_metalearning_more\models/100000_G.pth
  bicubic_G: ./pretrained_models/NewBasic/vimeo_last_S4.pth
  # bicubic_G: F:\DynaVSR-master\pretrained_models\NewBasic/vimeo_last.pth
  fixed_E: ./pretrained_models/MFDN/MFDN_Vimeo.pth
  pretrain_model_G: ./pretrained_models/EDVR/Vid4_G.pth
  pretrain_model_E: ./pretrained_models/EDVR/Vid4_E.pth
  strict_load: true
  resume_state: ~
  img_save_path: ../results

#### training settings: learning rate scheme, loss
train:
  lr_G: !!float 1e-4
  lr_scheme: MultiStepLR
  optim: Adam
  beta1: 0.9
  beta2: 0.99
  epochs: 10000
  niter: 3000000
  warmup_iter: -1  # -1: no warm up
  lr_steps: [100000, 200000]
  lr_gamma: 0.2
  loss_ftn: l1  # Loss function for updating KE model
  use_real: false

  pixel_criterion: cb
  pixel_weight: 1.0
  val_freq: !!float 5000
  
  maml:
    use_patch: false
    num_patch: 3
    patch_size: 64
    optimizer: Adam
    lr_alpha: !!float 1e-6
    beta1: 0.9
    beta2: 0.99
    adapt_iter: 1

  manual_seed: 0

#### logger
logger:
  print_freq: 200
  save_checkpoint_freq: !!float 5000